



clear all;
close all;
clc;

 Q1: Data Preprocessing 

 1.1 Load and inspect the dataset 
fprintf('Q1.1: Loading and Inspecting Dataset\n');
fprintf('=====================================\n');

% Load the dataset
data = readtable('customer_data.csv');

% Display first few rows
fprintf('First 5 rows of the dataset:\n');
disp(head(data, 5));

% Display summary statistics
fprintf('\nSummary Statistics:\n');
summary(data);

% Identify missing values
missing_values = sum(ismissing(data));
fprintf('\nMissing values in each column:\n');
disp(missing_values);

fprintf('\nTotal missing values: %d\n\n', sum(missing_values));

%% 1.2 Handle missing values (5 marks)
fprintf('Q1.2: Handling Missing Values\n');
fprintf('==============================\n');

% Fill missing values using mean for numeric columns
data = fillmissing(data, 'constant', 'MissingLocations', ismissing(data));

% Alternative: Use mean imputation
data.Age = fillmissing(data.Age, 'movmean', 3);
data.Income = fillmissing(data.Income, 'movmean', 3);
data.SpendingScore = fillmissing(data.SpendingScore, 'movmean', 3);

% Verify no missing values remain
fprintf('Missing values after handling:\n');
disp(sum(ismissing(data)));
fprintf('\n');

%% 1.3 Convert categorical variable to numeric 
fprintf('Q1.3: Converting Categorical Variable\n');
fprintf('======================================\n');

% Convert Gender to numeric using grp2idx
[gender_numeric, gender_labels] = grp2idx(data.Gender);

% Add numeric gender column
data.GenderNumeric = gender_numeric;

% Remove original Gender column
data.Gender = [];

fprintf('Gender encoding:\n');
for i = 1:length(gender_labels)
    fprintf('%s -> %d\n', gender_labels{i}, i);
end
fprintf('\n');

%% 1.4 Feature scaling (10 marks)
fprintf('Q1.4: Feature Scaling\n');
fprintf('=====================\n');

% Extract features for scaling (exclude target variable)
features = {'Age', 'Income', 'SpendingScore', 'GenderNumeric'};

% Standardization using zscore
for i = 1:length(features)
    data.(features{i}) = zscore(data.(features{i}));
end

fprintf('Features scaled using z-score standardization\n');
fprintf('First 5 rows after scaling:\n');
disp(head(data, 5));
fprintf('\n');

%% 1.5 Split dataset into training and testing sets 
fprintf('Q1.5: Splitting Dataset\n');
fprintf('=======================\n');

% Convert table to matrix for easier handling
data_matrix = table2array(data);

% Create partition: 70% training, 30% testing
cv = cvpartition(size(data_matrix, 1), 'HoldOut', 0.3);

% Split the data
train_data = data_matrix(training(cv), :);
test_data = data_matrix(test(cv), :);

fprintf('Training set size: %d samples\n', size(train_data, 1));
fprintf('Testing set size: %d samples\n\n', size(test_data, 1));
______________________________________________________________________
 Q2: Classification 

%% 2.1 Train kNN classifier 
fprintf('Q2: Classification\n');
fprintf('==================\n\n');
fprintf('Q2.1: k-Nearest Neighbors Classification\n');
fprintf('=========================================\n');

% Separate features and labels for training
X_train = train_data(:, 1:end-1);
y_train = train_data(:, end);

% Separate features and labels for testing
X_test = test_data(:, 1:end-1);
y_test = test_data(:, end);

% Test different k values
k_values = [3, 5, 7];
knn_accuracies = zeros(length(k_values), 1);
knn_conf_matrices = cell(length(k_values), 1);

for i = 1:length(k_values)
    k = k_values(i);
    
    % Train kNN model
    knn_model = fitcknn(X_train, y_train, 'NumNeighbors', k);
    
    % Predict on test set
    y_pred_knn = predict(knn_model, X_test);
    
    % Calculate accuracy
    knn_accuracies(i) = sum(y_pred_knn == y_test) / length(y_test);
    
    % Confusion matrix
    knn_conf_matrices{i} = confusionmat(y_test, y_pred_knn);
    
    fprintf('k = %d:\n', k);
    fprintf('  Accuracy: %.2f%%\n', knn_accuracies(i) * 100);
    fprintf('  Confusion Matrix:\n');
    disp(knn_conf_matrices{i});
    fprintf('\n');
end

% Select best k
[best_knn_acc, best_k_idx] = max(knn_accuracies);
best_k = k_values(best_k_idx);
fprintf('Best k value: %d with accuracy: %.2f%%\n\n', best_k, best_knn_acc * 100);

%% 2.2 Train SVM classifier (20 marks)
fprintf('Q2.2: Support Vector Machine Classification\n');
fprintf('============================================\n');

% Train SVM with linear kernel
svm_model = fitcsvm(X_train, y_train, 'KernelFunction', 'linear');

% Predict on test set
y_pred_svm = predict(svm_model, X_test);

% Calculate accuracy
svm_accuracy = sum(y_pred_svm == y_test) / length(y_test);

% Confusion matrix
svm_conf_matrix = confusionmat(y_test, y_pred_svm);

fprintf('SVM (Linear Kernel):\n');
fprintf('  Accuracy: %.2f%%\n', svm_accuracy * 100);
fprintf('  Confusion Matrix:\n');
disp(svm_conf_matrix);
fprintf('\n');
_____________________________________________________________________________

Q3: Performance Evaluation 

%% 3.1 Compute classification accuracy 
fprintf('Q3: Performance Evaluation\n');
fprintf('==========================\n\n');
fprintf('Q3.1: Classification Accuracies\n');
fprintf('================================\n');

fprintf('kNN (k=%d) Accuracy: %.2f%%\n', best_k, best_knn_acc * 100);
fprintf('SVM (Linear) Accuracy: %.2f%%\n\n', svm_accuracy * 100);

%% 3.2 Display results in a table 
fprintf('Q3.2: Comparison Table\n');
fprintf('======================\n');

% Create comparison table
Classifier = {'kNN (k=3)'; 'kNN (k=5)'; 'kNN (k=7)'; 'SVM (Linear)'};
Accuracy = [knn_accuracies(1) * 100; knn_accuracies(2) * 100; ...
            knn_accuracies(3) * 100; svm_accuracy * 100];

results_table = table(Classifier, Accuracy);
disp(results_table);
fprintf('\n');

%% 3.3 Discussion (10 marks)
fprintf('Q3.3: Model Performance Discussion\n');
fprintf('===================================\n\n');

fprintf('DISCUSSION:\n');
fprintf('-----------\n');
if best_knn_acc > svm_accuracy
    fprintf('The kNN classifier (k=%d) performs better with %.2f%% accuracy\n', ...
            best_k, best_knn_acc * 100);
    fprintf('compared to SVM linear kernel with %.2f%% accuracy.\n\n', svm_accuracy * 100);
else
    fprintf('The SVM classifier with linear kernel performs better with %.2f%% accuracy\n', ...
            svm_accuracy * 100);
    fprintf('compared to kNN (k=%d) with %.2f%% accuracy.\n\n', best_k, best_knn_acc * 100);
end

fprintf('Reasons for performance differences:\n');
fprintf('1. Dataset Characteristics:\n');
fprintf('   - The dataset has %d features and %d samples\n', size(X_train, 2), size(train_data, 1));
fprintf('   - Feature scaling was applied which benefits both algorithms\n\n');

fprintf('2. kNN Performance:\n');
fprintf('   - kNN is a non-parametric method suitable for non-linear boundaries\n');
fprintf('   - Performance varies with k; smaller k can overfit, larger k can underfit\n');
fprintf('   - Sensitive to local data distribution\n\n');

fprintf('3. SVM Performance:\n');
fprintf('   - SVM with linear kernel works well for linearly separable data\n');
fprintf('   - Robust to outliers due to margin maximization\n');
fprintf('   - May underperform if decision boundary is highly non-linear\n\n');

fprintf('4. Recommendation:\n');
if best_knn_acc > svm_accuracy
    fprintf('   - Use kNN (k=%d) for this dataset\n', best_k);
    fprintf('   - Consider trying SVM with RBF kernel for potential improvement\n');
else
    fprintf('   - Use SVM with linear kernel for this dataset\n');
    fprintf('   - The linear boundary appears to separate the classes well\n');
end

fprintf('\n');
fprintf('=== END OF ANALYSIS ===\n');

% Save results to file
save('classification_results.mat', 'results_table', 'knn_conf_matrices', ...
     'svm_conf_matrix', 'best_k', 'best_knn_acc', 'svm_accuracy');

fprintf('\nResults saved to classification_results.mat\n');



